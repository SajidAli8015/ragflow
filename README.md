# ğŸ§  RagFlow

RAGFlow Chat is a clean, modular, production-style conversational AI application built with **Retrieval-Augmented Generation (RAG)**.  
It supports document-grounded conversations, multi-chat sessions, streaming responses, persona selection, and multilingual output â€” with a **cleanly separated backend and frontend**.

---

## ğŸ–¼ï¸ Application Preview

![RAGFlow Chat UI](assets/ui.png)

> Save your screenshot as `assets/ui.png` to render correctly on GitHub.

---

## âœ¨ Key Features

- ğŸ”¹ **Clean Architecture**
  - Fully separated backend and frontend
  - Backend can run independently (CLI / future API)
- ğŸ”¹ **Retrieval-Augmented Generation (RAG)**
  - Upload documents (PDF, TXT, DOCX, CSV)
  - Text chunking, embeddings, and FAISS vector search
  - Per-chat RAG isolation
- ğŸ”¹ **Multi-Chat Sessions**
  - Create, switch, and rename chats
  - Each chat maintains its own history and context
- ğŸ”¹ **Streaming Responses**
  - Token-by-token streaming for real-time UX
- ğŸ”¹ **Persona Selection**
  - Friendly Assistant
  - Formal Expert
  - Tech Support
- ğŸ”¹ **Multilingual Support**
  - English
  - Urdu
  - Arabic
- ğŸ”¹ **User Controls**
  - Enable / disable RAG per chat
  - Reset messages
  - Download chat history as `.txt`
- ğŸ”¹ **Backend-Only Interfaces**
  - Chat CLI
  - RAG CLI (document ingestion + querying without UI)

---

## ğŸ—‚ï¸ Project Structure (with file descriptions)

```text
ragflow-chat/
â”œâ”€â”€ backend/                       # Core backend (frontend-agnostic)
â”‚   â”œâ”€â”€ __init__.py                # Package marker
â”‚   â”œâ”€â”€ config.py                  # Central config (chunk sizes, model names, etc.)
â”‚   â”œâ”€â”€ model.py                   # LLM + embeddings initialization (Gemini)
â”‚   â”œâ”€â”€ graph.py                   # LangGraph workflow / orchestration
â”‚   â”œâ”€â”€ chat_service.py            # High-level streaming chat service (UI/CLI call this)
â”‚   â”œâ”€â”€ rag.py                     # Retrieval service (query â†’ relevant context)
â”‚   â””â”€â”€ document_rag.py            # Document ingestion: extract â†’ chunk â†’ embed â†’ FAISS
â”‚
â”œâ”€â”€ frontend/                      # Streamlit frontend (UI only)
â”‚   â”œâ”€â”€ streamlit_app.py           # UI entrypoint (wires sidebar + chat UI)
â”‚   â”œâ”€â”€ sidebar.py                 # Sidebar UI: chats list, RAG upload, settings, actions
â”‚   â”œâ”€â”€ chat_ui.py                 # Chat rendering + streaming response display
â”‚   â””â”€â”€ state.py                   # Session-state helpers (new chat, rename, safe indexes)
â”‚
â”œâ”€â”€ scripts/                       # Backend-only utilities (no Streamlit required)
â”‚   â”œâ”€â”€ chat_cli.py                # CLI chat (persona + language; no document RAG)
â”‚   â””â”€â”€ rag_cli.py                 # CLI RAG: loads a local document path, then Q&A
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ ui.png                     # Screenshot used by README (optional but recommended)
â”‚
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml               # Local API keys (DO NOT COMMIT)
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore                     # Ignore venv, secrets, caches, etc.
â””â”€â”€ README.md                      # Project documentation
```

---

## ğŸ§° Tech Stack

- **Python 3.11+**
- **Streamlit** (UI)
- **LangChain + LangGraph** (orchestration)
- **Google Gemini** (LLM + embeddings)
- **FAISS** (vector search)
- **PyPDF2 / python-docx / pandas** (document parsing)

---

## âš™ï¸ Setup

### 1) Create a virtual environment

```bash
python -m venv venv
```

Activate:

- **Windows (PowerShell)**
  ```powershell
  .\venv\Scripts\Activate.ps1
  ```
- **macOS / Linux**
  ```bash
  source venv/bin/activate
  ```

### 2) Install dependencies

```bash
pip install -r requirements.txt
```

### 3) Add secrets

Create:

`.streamlit/secrets.toml`

```toml
GOOGLE_API_KEY = "YOUR_GOOGLE_API_KEY"
```

> This file is ignored by Git and should stay local.

---

## â–¶ï¸ Run the Streamlit UI

```bash
streamlit run frontend/streamlit_app.py
```

Open:

- http://localhost:8501

---

## ğŸ’¬ Run Chat CLI (backend-only)

Runs the chatbot in your terminal (no Streamlit).

```bash
python scripts/chat_cli.py
```

---

## ğŸ§ª Run RAG CLI (backend-only)

This runs RAG entirely in your terminal.

```bash
python scripts/rag_cli.py
```

### How â€œdocument uploadâ€ works in the CLI

In `rag_cli.py`, you **provide a local file path** when prompted (thatâ€™s the â€œuploadâ€ step for CLI).  
Supported formats: `pdf`, `txt`, `docx`, `csv`

The CLI will then:
1. Read the file from disk
2. Extract text
3. Chunk text (overlapping chunks)
4. Create embeddings
5. Build an in-memory FAISS index
6. Let you ask questions grounded in the document

**Notes**
- Vector store is **in-memory** (not persisted)
- Each run creates a **fresh** index


## ğŸš€ Future Improvements

- Persist vector stores (disk/DB) per user/chat
- Add FastAPI backend for web/mobile clients
- Authentication + user accounts
- Dockerization / deployment templates
- Better document parsing (OCR for scanned PDFs)



## ğŸ‘¤ Author

Built by **Sajid Ali**  
Focused on clean architecture, RAG systems, and maintainable AI applications.
